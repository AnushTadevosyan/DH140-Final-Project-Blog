[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final_Project",
    "section": "",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 30, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/Topics.html",
    "href": "posts/Topics.html",
    "title": "Introduction",
    "section": "",
    "text": "Anush Tadevosyan\n08/04/2023\n“Exploring Crime in United States”\nIn this project, we will answer various question about crimes in US states for a wide range of years\nLink to Dataset: https://corgis-edu.github.io/corgis/csv/state_crime/\nLink to Forum “Crime in DC”: https://www.reddit.com/r/washingtondc/comments/xx6g4e/crime_in_dc/\nLink to Forum “Is New Hampshire safe for minorities?”: https://www.reddit.com/r/newhampshire/comments/wafenm/is_new_hampshire_safe_for_minorities/\nThe reason for choosing this particular dataset is my personal interest in crime stories and documentaries. I am curious to see what can we find and what will be the conclusions that we will land upon after final findings. The end goal of this project is to find the safest state and the most dangerous state in terms of crimes in United States (according to dataset) and to predict how the rate of crimes will change in those states in future!\n\n\nCode\n!pip install pmdarima\n\n\n\nMethods\nData Presentation & Exploration Basic Information about the dataset: The dataset classifies crimes into two groups: property & violent crimes. Property crime refers to burglary, larceny, and motor related crime while violent crime refers to assault, murder, rape, and robbery. These reports range from the year 1960 to 2019. With this in mind, let’s understand and explore our data.\nThe dataset has 21 columns and 3115 rows. As mentioned, the dataset covers the crimes in states from the year 1960 to 2019 and we can see that half of data is for years 1990-2019\n\nimport pandas as pd\nimport pmdarima as pm\nfrom pmdarima import auto_arima\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment import vader\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.stem.porter import PorterStemmer\n\nnltk.download('stopwords')\ncrime_data = pd.read_csv('/home/jovyan/DH140-Final-Project/state_crime.csv')\n\n[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\n\nCode\n#total data summary\ncrime_data.describe()\n\n\n\n\nCode\n#first major crime group summary: property\ncrime_data['Data.Totals.Property.All'].describe()\n\n\n\n\nCode\n#second major crime group summary: property\ncrime_data['Data.Totals.Violent.All'].describe()\n\n\nDescriptive Statistics: If we look at the mean number of property and violent crimes over the years, property crimes are more common. The mean rate of property crimes also exceeds that of violent crimes. This is good news to us as we can already tell that the crimes that directly, phsyically affect people are less than other types of crimes historically!\n\n\nCode\ncorr_year_property_crime = crime_data['Data.Population'].corr(crime_data['Data.Totals.Property.All'])\ncorr_year_property_crime\n\n\n\n\nCode\ncorr_year_violent_crime = crime_data['Data.Population'].corr(crime_data['Data.Totals.Violent.All'])\ncorr_year_violent_crime\n\n\nCorrelations: As we can see from the results, there is a strong correlation in the size of population and number of crimes. This indicates, that generally, as the population of a state increases, the total number of property/violent crimes tends to increase\n\n\nCode\nmissing_values_cd = crime_data.isnull()\nmissing_values_count = missing_values_cd.sum()\nprint(missing_values_count)\n\n\nMissing Values: As we can see that we have no missing values in our dataset by looking at the output of the cell above\n\nlabel_encoder = LabelEncoder()\ncrime_data['State_Encoded'] = label_encoder.fit_transform(crime_data['State'])\n\nstate_encoding_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n\n\nprint(state_encoding_mapping)\n\n{'Alabama': 0, 'Alaska': 1, 'Arizona': 2, 'Arkansas': 3, 'California': 4, 'Colorado': 5, 'Connecticut': 6, 'Delaware': 7, 'District of Columbia': 8, 'Florida': 9, 'Georgia': 10, 'Hawaii': 11, 'Idaho': 12, 'Illinois': 13, 'Indiana': 14, 'Iowa': 15, 'Kansas': 16, 'Kentucky': 17, 'Louisiana': 18, 'Maine': 19, 'Maryland': 20, 'Massachusetts': 21, 'Michigan': 22, 'Minnesota': 23, 'Mississippi': 24, 'Missouri': 25, 'Montana': 26, 'Nebraska': 27, 'Nevada': 28, 'New Hampshire': 29, 'New Jersey': 30, 'New Mexico': 31, 'New York': 32, 'North Carolina': 33, 'North Dakota': 34, 'Ohio': 35, 'Oklahoma': 36, 'Oregon': 37, 'Pennsylvania': 38, 'Rhode Island': 39, 'South Carolina': 40, 'South Dakota': 41, 'Tennessee': 42, 'Texas': 43, 'United States': 44, 'Utah': 45, 'Vermont': 46, 'Virginia': 47, 'Washington': 48, 'West Virginia': 49, 'Wisconsin': 50, 'Wyoming': 51}\n\n\nEncoding Categorical Values: Above, we encoded the variable state, as later, it might be useful when doing time-series analysis, maybe using additional data related to states etc\n\n\nCode\n# See how much data per state, can the dataset be skewed? how many rows per state?\nnumber_of_rows_for_state = []\nlist_of_state_names = []\n\nfor key,value in state_encoding_mapping.items():\n    list_of_state_names.append(key)\n    key_data = crime_data[crime_data['State'] == key]\n    number_of_rows_for_state.append(key_data.shape[0])\n\nplt.bar(state_encoding_mapping.values(),number_of_rows_for_state)\nplt.xlabel('State')\nplt.ylabel('Number of rows (info) for state')\nplt.title('States and Number of Rows for them')\nplt.show()\n\n\n\n\n\n\nAs we can see, there are 60 entries per state (besides New York) so we don’t have to worry about data being skewed or bias (disproportionally number rows for a state compared to other states). In this analysis we won’t consider 5 less entries to be bias for the purposes of this research\n\n\n#| code-fold: true\n#| output: true\n\n# Scatter plot: Property crimes vs Year vs Encoded States\nplt.figure(figsize=(10, 6))\nplt.scatter(crime_data['Year'], crime_data['Data.Rates.Property.All'], c=crime_data['State_Encoded'], cmap='viridis')\nplt.colorbar(label='Encoded State')\nplt.xlabel('Year')\nplt.ylabel('Property Crime Rates')\nplt.title('Scatter Plot: Year vs. Property Crime Numbers with Encoded State')\nplt.show()\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\nplt.scatter(crime_data['Year'], crime_data['Data.Rates.Violent.All'], c=crime_data['State_Encoded'], cmap='viridis')\nplt.colorbar(label='Encoded State')\nplt.xlabel('Year')\nplt.ylabel('Violent Crime Rates')\nplt.title('Scatter Plot: Year vs. Violent Crime Numbers with Encoded State')\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(10, 6))\ncrime_data[['Data.Rates.Property.All', 'Data.Rates.Violent.All']].boxplot()\nplt.title('Box Plot: Property Crime Rates vs. Violent Crime Rates')\nplt.ylabel('Rates per 100,000 population')\nplt.xticks([1, 2], ['Property Crime Rates', 'Violent Crime Rates'])\nplt.show()\n\n\n\n\n\nData Disrtibution & Outliers: Through scatter plots, call them outliers or states with significant crime, we can see that total rate of violent crimes and total rate of property crimes is significantly larger in one of the state, which is in this group - ‘Alabama’: 0, ‘Alaska’: 1, ‘Arizona’: 2, ‘Arkansas’: 3, ‘California’: 4, ‘Colorado’: 5, ‘Connecticut’: 6, ‘Delaware’: 7, ‘District of Columbia’: 8, ‘Florida’: 9, ‘Georgia’: 10, ‘Hawaii’: 11, ‘Idaho’: 12, ‘Illinois’: 13, ‘Indiana’: 14, ‘Iowa’: 15, ‘Kansas’: 16, ‘Kentucky’: 17, ‘Louisiana’: 18, ‘Maine’: 19, ‘Maryland’: 20 Through analysis later, we will figure out which state exactly that is.\nThrough box plot, we can see that over the years and over the states, the rate of property crimes is larger than the rate of violent crimes (rates per 100,000 population).We can also see that there is a fair amount of outliers for both of the crime rate, which suggests that there has been years and states, where during those years, that specific state was an “exception” compared to others and had a higher crime rate. Note that the outliers for violent crime rates are more tightly stacked together, suggesting that the rate of violent crimes faced way more exceptions than the rate of property crimes\nAnalytical Process: After having some understanding of our data and exploring it, it is time to dive into the actual analysis. Based on exploration, we figured out an important point about exceptional crime rate in some states and that of states that falls between 0-20 in the Encoded State list, has a significantly higher crime rate. Now, we will analyze the graphs for each of the states and proceed to answer the main questions of this analysis, for which, we will be using “weights” to determine the safety score for each state and afterwards, using ARIMA statistical model, we will predict the future rate of crimes for those states!\n\n\nResults\nWe will be using code to answer the following questions 1. For each state, analyze the total number of propery crimes and total number of violent crimes throughout the years of 1960-2020 2. For each state, analyze the rate of property crimes and rate of violent crime throughout the years 1960-2019. 3. Based on historical data, which state is the safest state and the most dangerous state? 4. What will be the rate of total property crimes and total violent crimes in those two states in 2050? 5. Support findings with reddit post analyzations\n\n# make a list of state names from crime_data\nstate_name_list = crime_data['State'].tolist()\n# get rid of duplicates\nstate_name_list = list(set(state_name_list))\n\nfor state in state_name_list:\n    state_data = crime_data[crime_data['State'] == state]\n    #create new figure and axes ax_1\n    figure, ax_1 = plt.subplots()\n    ax_1.plot(state_data['Year'], state_data['Data.Totals.Property.All'], label='Property crimes', color='blue')\n    ax_1.set_xlabel('Year')\n    ax_1.set_ylabel('Number of property crimes', color='blue')\n    #create twin axis ax_2\n    ax_2 = ax_1.twinx()\n    ax_2.plot(state_data['Year'], state_data['Data.Totals.Violent.All'], label='Violent crimes', color='red')\n    ax_2.set_ylabel('Number of violent crimes', color='red')\n    max_property = state_data['Data.Totals.Property.All'].max()\n    max_violent = state_data['Data.Totals.Violent.All'].max()\n    # determine y axis limit\n    y_max = max(max_property,max_violent)\n    #make sure both y axes have the same range\n    ax_1.set_ylim(0, y_max)\n    ax_2.set_ylim(0, y_max)\n    mean_population = state_data['Data.Population'].mean()\n    plt.title(f'{state} : Mean population over the years - {int(mean_population)}')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfor state in state_name_list:\n    state_data = crime_data[crime_data['State'] == state]\n    #create new figure and axes ax_1\n    figure, ax_1 = plt.subplots()\n    ax_1.plot(state_data['Year'], state_data['Data.Rates.Property.All'], label='Property crimes', color='blue')\n    ax_1.set_xlabel('Year')\n    ax_1.set_ylabel('Rate of property crimes', color='blue')\n    #create twin axis ax_2\n    ax_2 = ax_1.twinx()\n    ax_2.plot(state_data['Year'], state_data['Data.Rates.Violent.All'], label='Violent crimes', color='red')\n    ax_2.set_ylabel('Rate of violent crimes', color='red')\n    max_property = state_data['Data.Rates.Property.All'].max()\n    max_violent = state_data['Data.Rates.Violent.All'].max()\n    # determine y axis limit\n    y_max = max(max_property,max_violent)\n    #make sure both y axes have the same range\n    ax_1.set_ylim(0, y_max)\n    ax_2.set_ylim(0, y_max)\n    plt.title(f'{state}')\n    plt.show()\n\n  \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, after having visualization for each individual state, in case if you’re interested about a particular state, let’s find out the safest and most dangerous state according to data from years 1960-2019! I will be assigning weight of 70% to violent crimes since it involves human life, which is more precious than anything else, in this case - properties. To normalize data, we will be using MinMaxScaler from sklearn library\n\n#MinMaxScaler method\nweight_property = 0.3\nweight_violent = 0.7\n\ncrime_data_sklearn = crime_data.copy()\ncolumns = ['Data.Rates.Property.All','Data.Rates.Violent.All']\nfor col in columns:\n  crime_data_sklearn[col] = MinMaxScaler().fit_transform(np.array(crime_data_sklearn[col]).reshape(-1,1))\n\n  \ncrime_data['Safety score'] = (crime_data_sklearn['Data.Rates.Property.All'] * weight_property) + (crime_data_sklearn['Data.Rates.Violent.All'] * weight_violent)\n\nplt.bar(crime_data['State_Encoded'],crime_data['Safety score'])\nplt.title('Safery Score by State')\nplt.xlabel('Encoded States')\nplt.ylabel('Safery Score')\nplt.show()\n\nprint(f\"MinMaxScaler Method: The safest state according to historical data is {crime_data.loc[crime_data['Safety score'].idxmin(), 'State']} with a safety score of {crime_data['Safety score'].min()}\")\n\nprint(f\"MinMaxScaler Method: The most dangerous state according to historical data is {crime_data.loc[crime_data['Safety score'].idxmax(), 'State']} with a safety score of {crime_data['Safety score'].max()}\")\n\n\n\n\n\n\n\nMinMaxScaler Method: The safest state according to historical data is New Hampshire with a safety score of 0.004380198183234749\nMinMaxScaler Method: The most dangerous state according to historical data is District of Columbia with a safety score of 0.9774202930976619\n\n\nAs the result show, according to this dataset, by using rate of crime and weights given to crime type, the safest state is New Hampshire and the most dangerous state is District of Columbia!\nHistorical data can take us so far as we have seen from the graphs. Now let’s jump to year 2050 and try to find out how the rate of crimes will change in these two states in future. For the purposes of this analysis, we will choose to use Autoregressive Integrated Moving Average (ARIMA) statistical model\n\nnew_hampshire_data = crime_data[crime_data['State'] == 'New Hampshire']\nnew_hampshire_data.set_index('Year', inplace=True)\n\n# split data into training and test sets\n\n#property\ntraining_prop = new_hampshire_data.iloc[:-15, :]\ntest_prop = new_hampshire_data.iloc[-15:, :]\n\n#violence\ntraining_viol = new_hampshire_data.iloc[:-15, :]\ntest_viol = new_hampshire_data.iloc[-15:, :]\n\n# train the ARIMA model\nmodel_prop = auto_arima(y=training_prop['Data.Rates.Property.All'], seasonal=False)\nmodel_viol = auto_arima(y=training_viol['Data.Rates.Violent.All'], seasonal=False)\n\n# forecast for year 2050\nn_periods = 45\npredictions_prop = model_prop.predict(n_periods=n_periods)\npredictions_viol = model_viol.predict(n_periods=n_periods)\n# start from the year of test set\nprediction_index_prop = pd.date_range(start='2005', periods=n_periods, freq='Y')\nprediction_index_viol = pd.date_range(start='2005', periods=n_periods, freq='Y')\n\n# make 'data.rates.proprerty.all' into a list for error calculation\n#property\ntest_set_property_rates = test_prop['Data.Rates.Property.All'].tolist()\npredictions_set_property_rates = predictions_prop.tolist()\n\n#violence\ntest_set_violent_rates = test_viol['Data.Rates.Violent.All'].tolist()\npredictions_set_violent_rates = predictions_viol.tolist()\n\nsquared_diff_sum = 0\n\nfor test_value,pred_value in zip(test_set_property_rates,predictions_set_property_rates):\n  squared_diff = (pred_value - test_value) ** 2\n  squared_diff_sum += squared_diff\n\nrmse = math.sqrt(squared_diff_sum / len(test_set_property_rates))\nprint(\"Root Mean Squared Error for Property Rates:\", rmse)\n\n\nsquared_diff_sum_1 = 0\n\nfor test_value,pred_value in zip(test_set_violent_rates,predictions_set_violent_rates):\n  squared_diff_1 = (pred_value - test_value) ** 2\n  squared_diff_sum_1 += squared_diff_1\n\nrmse_1 = math.sqrt(squared_diff_sum_1 / len(test_set_violent_rates))\nprint(\"Root Mean Squared Error for Violent Rates:\", rmse_1)\n\n#-----Property Crimes\nfig,ax=plt.subplots()\nax = training_prop['Data.Rates.Property.All'].plot(legend = True,label='Training Data')\ntest_prop['Data.Rates.Property.All'].plot(legend = True,ax=ax, label = 'Test Data')\nax.plot(prediction_index_prop.year,predictions_prop,label = 'Predicted Data', color = 'green')\nax.legend()\nplt.grid(True)\nplt.title('New Hampshire: Predicted Rate of Total Property crimes in future - Year 2050')\nplt.xlabel('Year')\nplt.ylabel('Rate of Total Property Crimes')\nplt.show()\n\n#----Violent Crimes\nfig_1,ax_1=plt.subplots()\nax_1 = training_viol['Data.Rates.Violent.All'].plot(legend = True,label='Training Data')\ntest_viol['Data.Rates.Violent.All'].plot(legend = True,ax=ax_1, label = 'Test Data')\nax_1.plot(prediction_index_viol.year,predictions_viol,label = 'Predicted Data', color = 'green')\nax_1.legend()\nplt.grid(True)\nplt.title('New Hampshire: Predicted Rate of Total Violent crimes in future - Year 2050')\nplt.xlabel('Year')\nplt.ylabel('Rate of Total Violent Crimes')\nplt.show()\n\n\n/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n  return get_prediction_index(\n/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  return get_prediction_index(\n/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n  return get_prediction_index(\n\n\nRoot Mean Squared Error for Property Rates: 328.16340617059114\nRoot Mean Squared Error for Violent Rates: 27.454519967879005\n\n\n\n\n\n\n\n\n\n\nCode\ndc_data = crime_data[crime_data['State'] == 'District of Columbia']\ndc_data.set_index('Year', inplace=True)\n\n# split data into training and test sets\n\n#property\ntraining_prop = dc_data.iloc[:-15, :]\ntest_prop = dc_data.iloc[-15:, :]\n\n#violence\ntraining_viol = dc_data.iloc[:-15, :]\ntest_viol = dc_data.iloc[-15:, :]\n\n# train the ARIMA model\nmodel_prop = auto_arima(y=training_prop['Data.Rates.Property.All'], seasonal=False)\nmodel_viol = auto_arima(y=training_viol['Data.Rates.Violent.All'], seasonal=False)\n\n# forecast for year 2050\nn_periods = 45\npredictions_prop = model_prop.predict(n_periods=n_periods)\npredictions_viol = model_viol.predict(n_periods=n_periods)\n# start from the year of test set\nprediction_index_prop = pd.date_range(start='2005', periods=n_periods, freq='Y')\nprediction_index_viol = pd.date_range(start='2005', periods=n_periods, freq='Y')\n\n# make 'data.rates.proprerty.all' into a list for error calculation\n#property\ntest_set_property_rates = test_prop['Data.Rates.Property.All'].tolist()\npredictions_set_property_rates = predictions_prop.tolist()\n\n#violence\ntest_set_violent_rates = test_viol['Data.Rates.Violent.All'].tolist()\npredictions_set_violent_rates = predictions_viol.tolist()\n\nsquared_diff_sum = 0\n\nfor test_value,pred_value in zip(test_set_property_rates,predictions_set_property_rates):\n  squared_diff = (pred_value - test_value) ** 2\n  squared_diff_sum += squared_diff\n\nrmse = math.sqrt(squared_diff_sum / len(test_set_property_rates))\nprint(\"Root Mean Squared Error for Property Rates:\", rmse)\n\n\nsquared_diff_sum_1 = 0\n\nfor test_value,pred_value in zip(test_set_violent_rates,predictions_set_violent_rates):\n  squared_diff_1 = (pred_value - test_value) ** 2\n  squared_diff_sum_1 += squared_diff_1\n\nrmse_1 = math.sqrt(squared_diff_sum_1 / len(test_set_violent_rates))\nprint(\"Root Mean Squared Error for Violent Rates:\", rmse_1)\n\n#-----Property Crimes\nfig,ax=plt.subplots()\nax = training_prop['Data.Rates.Property.All'].plot(legend = True,label='Training Data')\ntest_prop['Data.Rates.Property.All'].plot(legend = True,ax=ax, label = 'Test Data')\nax.plot(prediction_index_prop.year,predictions_prop,label = 'Predicted Data', color = 'green')\nax.legend()\nplt.grid(True)\nplt.title('District of Columbia: Predicted Rate of Total Property crimes in future - Year 2050')\nplt.xlabel('Year')\nplt.ylabel('Rate of Total Property Crimes')\nplt.show()\n\n#----Violent Crimes\nfig_1,ax_1=plt.subplots()\nax_1 = training_viol['Data.Rates.Violent.All'].plot(legend = True,label='Training Data')\ntest_viol['Data.Rates.Violent.All'].plot(legend = True,ax=ax_1, label = 'Test Data')\nax_1.plot(prediction_index_viol.year,predictions_viol,label = 'Predicted Data', color = 'green')\nax_1.legend()\nplt.grid(True)\nplt.title('District of Columbia: Predicted Rate of Total Violent crimes in future - Year 2050')\nplt.xlabel('Year')\nplt.ylabel('Rate of Total Violent Crimes')\nplt.show()\n\n\nIt is important to note that we have to consider the Root Mean Squared Error when analyzing results. However, as you can see, our predictions follow the general trend as the test data in the beginning\nNow, let’s dive into Reddit and see what people say about District of Columbia and New Hampshire in terms of crime and safety!\n\n#website - url - https://www.reddit.com/r/washingtondc/comments/xx6g4e/crime_in_dc/\n\nwith open('../reddit_DC.txt', 'r') as file:\n    dc_reddit_content = file.read()\nsia = vader.SentimentIntensityAnalyzer()\ndc_reddit_word_token = word_tokenize(dc_reddit_content)\n\nmyStopWords = stopwords.words('english')\n#get rid of stopwords\ndc_reddit_content_nostop = [w for w in dc_reddit_word_token if w not in myStopWords]\n#get rid of numbers\ndc_reddit_cleaned = [w for w in dc_reddit_content_nostop if not w.isdigit()]\n\npos_word_list = []\nneg_word_list = []\n\nfor word in dc_reddit_cleaned:\n    score = sia.polarity_scores(word)\n    if score['compound'] &gt; 0:\n        pos_word_list.append(word)\n    elif score['compound'] &lt; 0:\n        neg_word_list.append(word)\n        \nstemmed_positive_words = [PorterStemmer().stem(w) for w in pos_word_list]\nmost_positive_words = nltk.FreqDist(stemmed_positive_words).most_common(15)\nmost_positive_words.reverse() #so its similiar to the graphs shown\n\n\nstemmed_negative_words = [PorterStemmer().stem(w) for w in neg_word_list]\nmost_negative_words = nltk.FreqDist(stemmed_negative_words).most_common(15)\nmost_negative_words.reverse()\n\n# Positive: horizontal bar plot\ndf = pd.DataFrame(most_positive_words, columns=['Positive Words', 'Frequency'])\ncolor = 'orange'\ndf.plot(x='Positive Words', y='Frequency', kind='barh', legend=False, color=color)\nplt.title('Positive Words')\nplt.xlabel('Frequency')\nplt.ylabel('Positive Words')\nplt.title('Crime in DC: Positive Words')\nplt.show()\n\n\n# Negative: horizontal bar plot\ndf_1 = pd.DataFrame(most_negative_words, columns=['Negative Words', 'Frequency'])\ncolor_1 = 'purple'\ndf_1.plot(x='Negative Words', y='Frequency', kind='barh', legend=False, color=color_1)\nplt.title('Negative Words')\nplt.xlabel('Frequency')\nplt.ylabel('Negative Words')\nplt.title('Crime in DC: Negative Words')\nplt.show()\n\n\n\n\n\n\n\n\n\nCode\n#website - url - https://www.reddit.com/r/newhampshire/comments/wafenm/is_new_hampshire_safe_for_minorities/\n\nwith open('../nh_reddit.txt', 'r') as file:\n    nh_reddit_content = file.read()\nnh_reddit_content = nh_reddit_content.lower()\n\nsia = vader.SentimentIntensityAnalyzer()\nnh_reddit_word_token = word_tokenize(nh_reddit_content)\n\nmyStopWords = stopwords.words('english')\n#get rid of stopwords\nnh_reddit_content_nostop = [w for w in nh_reddit_word_token if w not in myStopWords]\n#get rid of numbers\nnh_reddit_cleaned = [w for w in nh_reddit_content_nostop if not w.isdigit()]\n\npos_word_list = []\nneg_word_list = []\n\nfor word in nh_reddit_cleaned:\n    score = sia.polarity_scores(word)\n    if score['compound'] &gt; 0:\n        pos_word_list.append(word)\n    elif score['compound'] &lt; 0:\n        neg_word_list.append(word)\n        \nstemmed_positive_words = [PorterStemmer().stem(w) for w in pos_word_list]\nmost_positive_words = nltk.FreqDist(stemmed_positive_words).most_common(15)\nmost_positive_words.reverse() #so its similiar to the graphs shown\n\n\nstemmed_negative_words = [PorterStemmer().stem(w) for w in neg_word_list]\nmost_negative_words = nltk.FreqDist(stemmed_negative_words).most_common(15)\nmost_negative_words.reverse()\n\n# Positive: horizontal bar plot\ndf = pd.DataFrame(most_positive_words, columns=['Positive Words', 'Frequency'])\ncolor = 'orange'\ndf.plot(x='Positive Words', y='Frequency', kind='barh', legend=False, color=color)\nplt.title('Positive Words')\nplt.xlabel('Frequency')\nplt.ylabel('Positive Words')\nplt.title('Is New Hampshire safe for minorities: Positive Words')\nplt.show()\n\n\n# Negative: horizontal bar plot\ndf_1 = pd.DataFrame(most_negative_words, columns=['Negative Words', 'Frequency'])\ncolor_1 = 'purple'\ndf_1.plot(x='Negative Words', y='Frequency', kind='barh', legend=False, color=color_1)\nplt.title('Negative Words')\nplt.xlabel('Frequency')\nplt.ylabel('Negative Words')\nplt.title('Is New Hampshire safe for minorities: Negative Words')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDiscussion of Results\nBased on the graphs above, let’s discuss our findings. First, lets summarize what we found to be general trend for states, in terms of property and violent crimes.\nThe analysis of property and violent crime trends across various U.S. states reveals distinctive patterns in both peak periods and general trends. Notably, Louisiana and Missouri experienced peak property and violent crime rates during the 1990s, reflecting a significant prevalence of criminal activities during that decade. Another group of states, including Arkansas, Kentucky, and several others, witnessed peak crime rates in the 1980s to 2000s, indicating prolonged periods of heightened criminal activities. In contrast, states like Nevada, South Carolina, Georgia, and Michigan exhibited relatively stable or fluctuating crime rates, suggesting varying degrees of control over criminal incidents. Additionally, states such as Arizona, Utah, North Dakota, and Alabama displayed unique peak periods for either property or violent crimes. When considering general trends, a substantial number of states demonstrated a consistent decrease in both property and violent crime rates over the years, which shows successful efforts in crime prevention and law enforcement. Conversely, states like Arkansas (in terms of violent crimes) and Tennessee experienced an overall increase in violent crime rates, demonstrating ongoing challenges in maintaining public safety.\nNext, according to our analysis and historical data, by assigning more weight/importance to violent crime (0.7) as opposed to property crimes (0.3), as human lives matter much more than any property, we found out that the safest state is the state of New Hampshire, whereas on the other hand, according to our method of analysis, historically speaking, the most dangerous state is District of Columbia.\nBy conducting further analysis, we found that the total property crime rate will be slightly decreasing in New Hampshire in year 2050, and the total violent rate will stay the same. In constrast, for District of Columbia, luckily our predictions show that both rates will be decreasing slightly and then stabilizing. With this in mind, it is important to note that these results were obtained using our choice of statistical method and variables, however, results may vary depending on the algorithm chosen. One can refer to the Mean Squared Error to draw their own conclusions about the accuracy of the model.\nNext step of our analysis was diving into public forums to understand what people say about these states. The query googled to find the forums where the same - “safety in {state_name} reddit” and one of the results for each state was chosen. By conducting sentiment analsysis for the forum named “Crime in DC”, we see words like “violent, victim, assault, problem” to be prevelant. This suggests that people have discussed this issues and due to the existence of the words like “violent, assault, problem, victim, scary”, we can draw conclusion that there is an actual safety problem in DC as these words are specific, meaning that people most likely discussed crime cases, talking about victim, people who were assaulted and that crime is a problem in DC. Looking at the positive words, we see “safe, like, safer, better, thank”, which suggests that DC might be getting safer and better. In contrast, if we examine the forum “Is New Hampshire safe for minorities”, we see positive words like “safe, care, free, positive, funny, sure” which suggests that people generally refer that it is a safe state for minorities. Looking at the negative words, we see words like “racist, crime, attack, murder, problem”, which may suggests that people are discussing the issues of racism, discussing about prevalence of crime and more. However, we can see that the frequency of positive words considerably exceeds the frequency of negative words, which suggests that overall, people have a good opinion about safety of minorities in New Hampshire.\nAlthough, the choice of forums might be considered to be bias, it is important to note that with data science projects that involve humanstic content, such as cultural, social or other type of data, the completness of data and potential bias become crucial considerations. With that in mind, the choice of forums supports the findings in this particular analysis and by no means suggests that other results may not be obtained."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]